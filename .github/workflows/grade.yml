name: Grade Submission

on:
  push:
    branches: [main]
    paths:
      - 'solution/**'
  workflow_dispatch:

permissions:
  contents: read
  issues: write
  checks: write

env:
  PYTHON_VERSION: '3.11'

jobs:
  grade:
    name: Auto-Grade
    runs-on: ubuntu-latest
    timeout-minutes: 5

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install -q signalwire-agents pyyaml

      - name: Check solution exists
        id: check
        run: |
          if [ -f "solution/agent.py" ]; then
            if grep -q "AgentBase" solution/agent.py; then
              echo "exists=true" >> $GITHUB_OUTPUT
            else
              echo "exists=false" >> $GITHUB_OUTPUT
              echo "::warning::solution/agent.py does not contain AgentBase"
            fi
          else
            echo "exists=false" >> $GITHUB_OUTPUT
            echo "::warning::No solution/agent.py found"
          fi

      - name: Run grading and generate report
        if: steps.check.outputs.exists == 'true'
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          python << 'GRADESCRIPT'
          import subprocess
          import json
          import yaml
          import sys
          import os
          import urllib.request
          import urllib.error

          # Load test config
          with open('tests/grading.yaml') as f:
              config = yaml.safe_load(f)

          results = {
              'assignment': config['assignment'],
              'checks': [],
              'score': 0,
              'max_score': 0,
              'passed': False
          }

          agent_file = 'solution/agent.py'

          # Get SWML once for all checks that need it
          swml_result = subprocess.run(
              ['swaig-test', agent_file, '--dump-swml', '--raw'],
              capture_output=True, text=True, timeout=30
          )
          swml_data = None
          swml_text = ""
          if swml_result.returncode == 0:
              try:
                  swml_data = json.loads(swml_result.stdout)
                  swml_text = swml_result.stdout.lower()
              except:
                  pass

          for check in config['checks']:
              check_result = {
                  'id': check['id'],
                  'name': check['name'],
                  'max_points': check['points'],
                  'points': 0,
                  'passed': False,
                  'output': ''
              }

              try:
                  if check['type'] == 'instantiate':
                      result = subprocess.run(
                          ['swaig-test', agent_file, '--list-tools'],
                          capture_output=True, text=True, timeout=30
                      )
                      check_result['passed'] = result.returncode == 0
                      if result.returncode != 0:
                          check_result['output'] = result.stderr[:200]

                  elif check['type'] == 'swml_valid':
                      if swml_data:
                          check_result['passed'] = True
                          for req in check.get('require', []):
                              path = req.get('path', '')
                              value = swml_data
                              for part in path.replace('[', '.').replace(']', '').split('.'):
                                  if not part:
                                      continue
                                  if part.isdigit():
                                      try:
                                          value = value[int(part)]
                                      except (IndexError, KeyError, TypeError):
                                          value = None
                                          break
                                  else:
                                      try:
                                          value = value.get(part)
                                      except (AttributeError, TypeError):
                                          value = None
                                          break
                              if value is None:
                                  check_result['passed'] = False
                                  check_result['output'] = f"Missing: {path}"
                                  break
                      else:
                          check_result['output'] = 'Could not generate SWML'

                  elif check['type'] == 'swml_contains':
                      if swml_text:
                          check_result['passed'] = True
                          for req in check.get('require', []):
                              text = req.get('text', '').lower()
                              if text and text not in swml_text:
                                  check_result['passed'] = False
                                  check_result['output'] = f"'{text}' not found in SWML"
                                  break
                      else:
                          check_result['output'] = 'Could not generate SWML'

              except subprocess.TimeoutExpired:
                  check_result['output'] = 'Timeout exceeded'
              except Exception as e:
                  check_result['output'] = str(e)[:200]

              if check_result['passed']:
                  check_result['points'] = check['points']

              results['checks'].append(check_result)

          # Calculate totals
          results['max_score'] = sum(c['max_points'] for c in results['checks'])
          results['score'] = sum(c['points'] for c in results['checks'])
          results['percentage'] = round(results['score'] / results['max_score'] * 100, 1) if results['max_score'] > 0 else 0
          results['passed'] = results['percentage'] >= config['assignment']['passing_score']

          # Save results
          with open('results.json', 'w') as f:
              json.dump(results, f, indent=2)

          print(f"Score: {results['score']}/{results['max_score']} ({results['percentage']}%)")
          print(f"Status: {'PASSED' if results['passed'] else 'NOT PASSING'}")

          # Generate markdown report
          report = f"## Grading Results\n\n"
          report += f"**Assignment:** {results['assignment']['name']}\n"
          report += f"**Score:** {results['score']}/{results['max_score']} ({results['percentage']}%)\n"
          report += f"**Status:** {'PASSED' if results['passed'] else 'NOT PASSING'}\n\n"

          report += "### Checks\n\n"
          report += "| # | Check | Points | Status |\n"
          report += "|---|-------|--------|--------|\n"

          for i, check in enumerate(results['checks']):
              status = '[x]' if check['passed'] else '[ ]'
              report += f"| {i+1} | {check['name']} | {check['points']}/{check['max_points']} | {status} |\n"

          # Add feedback for failed checks
          failed = [c for c in results['checks'] if not c['passed'] and c['output']]
          if failed:
              report += "\n### Details\n\n"
              for check in failed:
                  report += f"**{check['name']}:** {check['output']}\n\n"

          # Add pass/fail message
          report += "\n---\n\n"
          if results['passed']:
              report += config.get('feedback', {}).get('pass', 'Congratulations!')
          else:
              report += config.get('feedback', {}).get('fail', 'Please review and try again.')

          from datetime import datetime
          report += f"\n\n---\n*Graded: {datetime.utcnow().isoformat()}Z*"

          # Post to GitHub Issues
          github_token = os.environ.get('GITHUB_TOKEN')
          repo_full = os.environ.get('GITHUB_REPOSITORY', '')

          if github_token and repo_full:
              api_base = "https://api.github.com"
              headers = {
                  "Authorization": f"Bearer {github_token}",
                  "Accept": "application/vnd.github+json",
                  "X-GitHub-Api-Version": "2022-11-28"
              }

              # Check for existing grading issue
              list_url = f"{api_base}/repos/{repo_full}/issues?labels=grading&state=open"
              req = urllib.request.Request(list_url, headers=headers)
              try:
                  with urllib.request.urlopen(req) as resp:
                      issues = json.loads(resp.read().decode())

                  if issues:
                      issue_num = issues[0]['number']
                      comment_url = f"{api_base}/repos/{repo_full}/issues/{issue_num}/comments"
                      data = json.dumps({"body": report}).encode()
                      req = urllib.request.Request(comment_url, data=data, headers={**headers, "Content-Type": "application/json"})
                      with urllib.request.urlopen(req) as resp:
                          print(f"Added comment to issue #{issue_num}")
                  else:
                      create_url = f"{api_base}/repos/{repo_full}/issues"
                      data = json.dumps({
                          "title": "Grading Results",
                          "body": report,
                          "labels": ["grading"]
                      }).encode()
                      req = urllib.request.Request(create_url, data=data, headers={**headers, "Content-Type": "application/json"})
                      with urllib.request.urlopen(req) as resp:
                          result = json.loads(resp.read().decode())
                          print(f"Created issue #{result['number']}")

              except urllib.error.HTTPError as e:
                  print(f"GitHub API error: {e.code} - {e.read().decode()[:200]}")
              except Exception as e:
                  print(f"Error posting to GitHub: {e}")

          GRADESCRIPT

      - name: Upload results
        if: steps.check.outputs.exists == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: grading-results
          path: results.json
          retention-days: 90
